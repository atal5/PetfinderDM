{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns',500)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the train and test cleaned data with engineered features\n",
    "train_copy_df = pd.read_csv('train_complete.csv')\n",
    "test_copy_df = pd.read_csv('test_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>DescrLength</th>\n",
       "      <th>Description</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Fee</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Health</th>\n",
       "      <th>IsHealthy</th>\n",
       "      <th>IsMixedBreed</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>Name</th>\n",
       "      <th>NameFrequency</th>\n",
       "      <th>NumColors</th>\n",
       "      <th>PetID</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>RescuerID</th>\n",
       "      <th>Score</th>\n",
       "      <th>State</th>\n",
       "      <th>StateLevel</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Type</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>RescuerNumPosts</th>\n",
       "      <th>BreedRank</th>\n",
       "      <th>detectionConfidence</th>\n",
       "      <th>joyLikelihood</th>\n",
       "      <th>sorrowLikelihood</th>\n",
       "      <th>angerLikelihood</th>\n",
       "      <th>surpriseLikelihood</th>\n",
       "      <th>underExposedLikelihood</th>\n",
       "      <th>blurredLikelihood</th>\n",
       "      <th>headwearLikelihood</th>\n",
       "      <th>labels</th>\n",
       "      <th>street dog</th>\n",
       "      <th>dog like mammal</th>\n",
       "      <th>aegean cat</th>\n",
       "      <th>carnivoran</th>\n",
       "      <th>small to medium sized cats</th>\n",
       "      <th>cat like mammal</th>\n",
       "      <th>dog breed</th>\n",
       "      <th>snout</th>\n",
       "      <th>whiskers</th>\n",
       "      <th>domestic short haired cat</th>\n",
       "      <th>puppy</th>\n",
       "      <th>dog breed group</th>\n",
       "      <th>fauna</th>\n",
       "      <th>kitten</th>\n",
       "      <th>european shorthair</th>\n",
       "      <th>dog</th>\n",
       "      <th>sporting group</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "      <td>Nibble</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>86e1089a3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3734</td>\n",
       "      <td>0.3</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919535</td>\n",
       "      <td>0.894983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680083</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>No Name Yet</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>6296e909a</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1408</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>41401</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891908</td>\n",
       "      <td>0.834615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.672995</td>\n",
       "      <td>0.905297</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658263</td>\n",
       "      <td>0.841606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.981269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2</td>\n",
       "      <td>Brisco</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3422e4906</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6965</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941017</td>\n",
       "      <td>0.616009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.720260</td>\n",
       "      <td>0.903123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>Miko</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5842f1ff5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4107</td>\n",
       "      <td>0.9</td>\n",
       "      <td>41401</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.583048</td>\n",
       "      <td>0.946035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938040</td>\n",
       "      <td>0.677403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.858104</td>\n",
       "      <td>0.874950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978698</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>850a43f90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4191</td>\n",
       "      <td>0.6</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942495</td>\n",
       "      <td>0.783549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.932304</td>\n",
       "      <td>0.902654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed  Age  Breed1  Breed2  Color1  Color2  Color3  DescrLength  \\\n",
       "0              2    3     299       0       1       7       0          359   \n",
       "1              0    1     265       0       1       2       0          118   \n",
       "2              3    1     307       0       2       7       0          393   \n",
       "3              2    4     307       0       1       2       0          146   \n",
       "4              2    1     307       0       1       0       0          390   \n",
       "\n",
       "                                         Description  Dewormed  Fee  \\\n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         2  100   \n",
       "1  I just found it alone yesterday near my apartm...         3    0   \n",
       "2  Their pregnant mother was dumped by her irresp...         1    0   \n",
       "3  Good guard dog, very alert, active, obedience ...         1  150   \n",
       "4  This handsome yet cute boy is up for adoption....         2    0   \n",
       "\n",
       "   FurLength  Gender  Health  IsHealthy  IsMixedBreed  Magnitude  \\\n",
       "0          1       1       1          7             0        2.4   \n",
       "1          2       1       1         10             0        0.7   \n",
       "2          2       1       1          5             0        3.7   \n",
       "3          1       2       1          5             0        0.9   \n",
       "4          1       1       1          7             0        3.7   \n",
       "\n",
       "   MaturitySize         Name  NameFrequency  NumColors      PetID  PhotoAmt  \\\n",
       "0             1       Nibble              1          2  86e1089a3         1   \n",
       "1             2  No Name Yet             22          2  6296e909a         2   \n",
       "2             2       Brisco              1          2  3422e4906         7   \n",
       "3             2         Miko             11          2  5842f1ff5         8   \n",
       "4             2       Hunter              5          1  850a43f90         3   \n",
       "\n",
       "   Quantity  RescuerID  Score  State  StateLevel  Sterilized  Type  \\\n",
       "0         1       3734    0.3  41326           0           2     2   \n",
       "1         1       1408   -0.2  41401           1           3     2   \n",
       "2         1       6965    0.2  41326           0           2     1   \n",
       "3         1       4107    0.9  41401           1           2     1   \n",
       "4         1       4191    0.6  41326           0           2     1   \n",
       "\n",
       "   Vaccinated  VideoAmt  WordCount  RescuerNumPosts  BreedRank  \\\n",
       "0           2         0         69                8          4   \n",
       "1           3         0         23                1          3   \n",
       "2           1         0         69              459          1   \n",
       "3           1         0         25               50          1   \n",
       "4           2         0         81              134          1   \n",
       "\n",
       "   detectionConfidence  joyLikelihood  sorrowLikelihood  angerLikelihood  \\\n",
       "0                  0.0            0.0               0.0              0.0   \n",
       "1                  0.0            0.0               0.0              0.0   \n",
       "2                  0.0            0.0               0.0              0.0   \n",
       "3                  0.0            0.0               0.0              0.0   \n",
       "4                  0.0            0.0               0.0              0.0   \n",
       "\n",
       "   surpriseLikelihood  underExposedLikelihood  blurredLikelihood  \\\n",
       "0                 0.0                     0.0                0.0   \n",
       "1                 0.0                     0.0                0.0   \n",
       "2                 0.0                     0.0                0.0   \n",
       "3                 0.0                     0.0                0.0   \n",
       "4                 0.0                     0.0                0.0   \n",
       "\n",
       "   headwearLikelihood  labels  street dog  dog like mammal  aegean cat  \\\n",
       "0                 0.0     9.0    0.000000         0.000000         0.0   \n",
       "1                 0.0    10.0    0.000000         0.000000         0.0   \n",
       "2                 0.0    10.0    0.000000         0.941660         0.0   \n",
       "3                 0.0    10.0    0.583048         0.946035         0.0   \n",
       "4                 0.0    10.0    0.000000         0.965755         0.0   \n",
       "\n",
       "   carnivoran  small to medium sized cats  cat like mammal  dog breed  \\\n",
       "0    0.000000                    0.919535         0.894983   0.000000   \n",
       "1    0.000000                    0.891908         0.834615   0.000000   \n",
       "2    0.649802                    0.000000         0.000000   0.941017   \n",
       "3    0.655879                    0.000000         0.000000   0.938040   \n",
       "4    0.661956                    0.000000         0.000000   0.942495   \n",
       "\n",
       "      snout  whiskers  domestic short haired cat     puppy  dog breed group  \\\n",
       "0  0.000000  0.680083                       0.74  0.000000         0.000000   \n",
       "1  0.672995  0.905297                       0.00  0.000000         0.658263   \n",
       "2  0.616009  0.000000                       0.00  0.720260         0.903123   \n",
       "3  0.677403  0.000000                       0.00  0.858104         0.874950   \n",
       "4  0.783549  0.000000                       0.00  0.932304         0.902654   \n",
       "\n",
       "      fauna    kitten  european shorthair       dog  sporting group       cat  \n",
       "0  0.000000  0.695694                 0.0  0.000000        0.000000  0.990786  \n",
       "1  0.841606  0.000000                 0.0  0.000000        0.000000  0.981269  \n",
       "2  0.000000  0.000000                 0.0  0.960457        0.000000  0.000000  \n",
       "3  0.000000  0.000000                 0.0  0.978698        0.546364  0.000000  \n",
       "4  0.000000  0.000000                 0.0  0.984346        0.000000  0.000000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "std_quantity = scaler.fit_transform(train_copy_df['Quantity'].values.reshape(-1,1))\n",
    "train_copy_df['NormQuantity'] = std_quantity\n",
    "\n",
    "std_quantity = scaler.fit_transform(test_copy_df['Quantity'].values.reshape(-1,1))\n",
    "test_copy_df['NormQuantity'] = std_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['AdoptionSpeed','Description','Name','PetID','Quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 57) (14993,) (3948, 57)\n"
     ]
    }
   ],
   "source": [
    "#Drop description, petid, name\n",
    "y = train_copy_df['AdoptionSpeed']\n",
    "X = train_copy_df.drop(drop_cols,axis=1)\n",
    "test_X = test_copy_df.drop(drop_cols, axis=1)\n",
    "print(X.shape,y.shape,test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve,GroupKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what will be the final predicted train and test values\n",
    "train_meta = np.zeros(y.shape)\n",
    "test_meta = np.zeros(test_X.shape[0])\n",
    "\n",
    "#SMOTE over samplingg\n",
    "#sm = SMOTE(random_state=12, kind='regular')\n",
    "\n",
    "# Choose and initialize a model.\n",
    "clf = XGBClassifier(max_depth=7,n_estimators=250,n_jobs=4)\n",
    "\n",
    "# Divide the training data into k-folds, k=4 here.\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=1812).split(X, y))\n",
    "\n",
    "#splits = list(GroupKFold(n_splits=4).split(X, y, X['RescuerID']))\n",
    "\n",
    "# Loop over the folds and fit the model to the fold's training data.\n",
    "# Then evaluate that model on i) the validation data of that fold, \n",
    "# and ii) on all of the test data.\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        # The training and validation sets for this fold\n",
    "        X_train = X.iloc[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_val = X.iloc[valid_idx]\n",
    "        y_val = y[valid_idx]\n",
    "        \n",
    "        #x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "        #X_train,y_train = x_train_res,y_train_res\n",
    "        \n",
    "        # Fit the model\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Look at the validation kappa and accuracy with classes right from the model\n",
    "        y_pred = clf.predict(X_val)\n",
    "        print(\"Fold {}: accuracy = {:.1f}%, kappa = {:.4f}  (no boundary adjustment)\".format(idx,\n",
    "                                100.0*accuracy_score(y_val, y_pred),     \n",
    "                                cohen_kappa_score(y_val, y_pred, weights='quadratic')))\n",
    "        #\n",
    "        # Assign real-valued classes in addition to the integer classes of y_pred.\n",
    "        # Start with the predicted probabilities by class\n",
    "        y_probs = clf.predict_proba(X_val)\n",
    "        # and get the class values (use a copy incase we change values)\n",
    "        class_vals = clf.classes_.copy()\n",
    "        # Change the ordinal weight of class 0 to be -1 as suggested by the plot in discussion:\n",
    "        # https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76265\n",
    "        # Does mot make much difference, though.\n",
    "        class_vals[0] = -1\n",
    "        # Create the float class values as the probability-weighted class\n",
    "        # Here a python \"list comprehension\" is used rather than a loop.\n",
    "        y_floats = [sum(y_probs[ix]*class_vals) for ix in range(len(y_probs[:,0]))]\n",
    "        #   \n",
    "        # Save these y_float values instead of the y_pred integers;\n",
    "        ##train_meta[valid_idx] = y_pred.reshape(-1)\n",
    "        train_meta[valid_idx] = y_floats\n",
    "        # the predictions for just this validation fold are saved in the train_meta array;\n",
    "        # looping over all folds will provide one prediction for each training sample.\n",
    "\n",
    "        # Now use this fold's same model to generate Test predictions.\n",
    "        ##y_test = clf.predict(test_X)\n",
    "        # Instead of integer classes, get the predicted probabilites\n",
    "        test_probs = clf.predict_proba(test_X)\n",
    "        # and turn these into float class values.\n",
    "        # Unlike the validation case, we get a test prediction from every fold,\n",
    "        # so those float predictions are averaged. python list comprehension is used again.\n",
    "        ##test_meta += y_test.reshape(-1) / len(splits)\n",
    "        test_meta += np.array([sum(test_probs[ix]*class_vals) for\n",
    "                               ix in range(len(test_probs[:,0]))]) / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depth 7\n",
    "# Fold 0: accuracy = 43.9%, kappa = 0.3997  (no boundary adjustment)\n",
    "# Fold 1: accuracy = 43.4%, kappa = 0.4013  (no boundary adjustment)\n",
    "# Fold 2: accuracy = 43.6%, kappa = 0.3733  (no boundary adjustment)\n",
    "# Fold 3: accuracy = 43.5%, kappa = 0.3724  (no boundary adjustment)\n",
    "\n",
    "#depth 9\n",
    "# Fold 0: accuracy = 44.3%, kappa = 0.3917  (no boundary adjustment)\n",
    "# Fold 1: accuracy = 44.3%, kappa = 0.4062  (no boundary adjustment)\n",
    "# Fold 2: accuracy = 42.0%, kappa = 0.3699  (no boundary adjustment)\n",
    "# Fold 3: accuracy = 44.5%, kappa = 0.3919  (no boundary adjustment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[1]\tvalid_0's multi_logloss: 1.45359\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 1.44361\n",
      "[3]\tvalid_0's multi_logloss: 1.43419\n",
      "[4]\tvalid_0's multi_logloss: 1.42594\n",
      "[5]\tvalid_0's multi_logloss: 1.41785\n",
      "[6]\tvalid_0's multi_logloss: 1.41068\n",
      "[7]\tvalid_0's multi_logloss: 1.40418\n",
      "[8]\tvalid_0's multi_logloss: 1.39875\n",
      "[9]\tvalid_0's multi_logloss: 1.39318\n",
      "[10]\tvalid_0's multi_logloss: 1.38846\n",
      "[11]\tvalid_0's multi_logloss: 1.38456\n",
      "[12]\tvalid_0's multi_logloss: 1.38035\n",
      "[13]\tvalid_0's multi_logloss: 1.37698\n",
      "[14]\tvalid_0's multi_logloss: 1.37359\n",
      "[15]\tvalid_0's multi_logloss: 1.37046\n",
      "[16]\tvalid_0's multi_logloss: 1.36742\n",
      "[17]\tvalid_0's multi_logloss: 1.36485\n",
      "[18]\tvalid_0's multi_logloss: 1.3623\n",
      "[19]\tvalid_0's multi_logloss: 1.36035\n",
      "[20]\tvalid_0's multi_logloss: 1.35801\n",
      "[21]\tvalid_0's multi_logloss: 1.35553\n",
      "[22]\tvalid_0's multi_logloss: 1.35301\n",
      "[23]\tvalid_0's multi_logloss: 1.35086\n",
      "[24]\tvalid_0's multi_logloss: 1.34875\n",
      "[25]\tvalid_0's multi_logloss: 1.3473\n",
      "[26]\tvalid_0's multi_logloss: 1.34528\n",
      "[27]\tvalid_0's multi_logloss: 1.34406\n",
      "[28]\tvalid_0's multi_logloss: 1.34237\n",
      "[29]\tvalid_0's multi_logloss: 1.34054\n",
      "[30]\tvalid_0's multi_logloss: 1.33916\n",
      "[31]\tvalid_0's multi_logloss: 1.33792\n",
      "[32]\tvalid_0's multi_logloss: 1.33678\n",
      "[33]\tvalid_0's multi_logloss: 1.33531\n",
      "[34]\tvalid_0's multi_logloss: 1.33434\n",
      "[35]\tvalid_0's multi_logloss: 1.33275\n",
      "[36]\tvalid_0's multi_logloss: 1.33132\n",
      "[37]\tvalid_0's multi_logloss: 1.32995\n",
      "[38]\tvalid_0's multi_logloss: 1.32887\n",
      "[39]\tvalid_0's multi_logloss: 1.3279\n",
      "[40]\tvalid_0's multi_logloss: 1.32679\n",
      "[41]\tvalid_0's multi_logloss: 1.32547\n",
      "[42]\tvalid_0's multi_logloss: 1.32475\n",
      "[43]\tvalid_0's multi_logloss: 1.32364\n",
      "[44]\tvalid_0's multi_logloss: 1.32259\n",
      "[45]\tvalid_0's multi_logloss: 1.32187\n",
      "[46]\tvalid_0's multi_logloss: 1.32084\n",
      "[47]\tvalid_0's multi_logloss: 1.32001\n",
      "[48]\tvalid_0's multi_logloss: 1.31923\n",
      "[49]\tvalid_0's multi_logloss: 1.31863\n",
      "[50]\tvalid_0's multi_logloss: 1.31798\n",
      "[51]\tvalid_0's multi_logloss: 1.31731\n",
      "[52]\tvalid_0's multi_logloss: 1.31618\n",
      "[53]\tvalid_0's multi_logloss: 1.31532\n",
      "[54]\tvalid_0's multi_logloss: 1.31458\n",
      "[55]\tvalid_0's multi_logloss: 1.31406\n",
      "[56]\tvalid_0's multi_logloss: 1.31374\n",
      "[57]\tvalid_0's multi_logloss: 1.31322\n",
      "[58]\tvalid_0's multi_logloss: 1.31249\n",
      "[59]\tvalid_0's multi_logloss: 1.31212\n",
      "[60]\tvalid_0's multi_logloss: 1.31154\n",
      "[61]\tvalid_0's multi_logloss: 1.31091\n",
      "[62]\tvalid_0's multi_logloss: 1.31068\n",
      "[63]\tvalid_0's multi_logloss: 1.3102\n",
      "[64]\tvalid_0's multi_logloss: 1.30975\n",
      "[65]\tvalid_0's multi_logloss: 1.3094\n",
      "[66]\tvalid_0's multi_logloss: 1.30919\n",
      "[67]\tvalid_0's multi_logloss: 1.30852\n",
      "[68]\tvalid_0's multi_logloss: 1.30813\n",
      "[69]\tvalid_0's multi_logloss: 1.30776\n",
      "[70]\tvalid_0's multi_logloss: 1.30738\n",
      "[71]\tvalid_0's multi_logloss: 1.30677\n",
      "[72]\tvalid_0's multi_logloss: 1.30634\n",
      "[73]\tvalid_0's multi_logloss: 1.30587\n",
      "[74]\tvalid_0's multi_logloss: 1.30579\n",
      "[75]\tvalid_0's multi_logloss: 1.30545\n",
      "[76]\tvalid_0's multi_logloss: 1.30502\n",
      "[77]\tvalid_0's multi_logloss: 1.30484\n",
      "[78]\tvalid_0's multi_logloss: 1.30474\n",
      "[79]\tvalid_0's multi_logloss: 1.30444\n",
      "[80]\tvalid_0's multi_logloss: 1.30404\n",
      "[81]\tvalid_0's multi_logloss: 1.30317\n",
      "[82]\tvalid_0's multi_logloss: 1.30296\n",
      "[83]\tvalid_0's multi_logloss: 1.30235\n",
      "[84]\tvalid_0's multi_logloss: 1.30226\n",
      "[85]\tvalid_0's multi_logloss: 1.30216\n",
      "[86]\tvalid_0's multi_logloss: 1.30168\n",
      "[87]\tvalid_0's multi_logloss: 1.30138\n",
      "[88]\tvalid_0's multi_logloss: 1.3011\n",
      "[89]\tvalid_0's multi_logloss: 1.30065\n",
      "[90]\tvalid_0's multi_logloss: 1.30038\n",
      "[91]\tvalid_0's multi_logloss: 1.30009\n",
      "[92]\tvalid_0's multi_logloss: 1.30018\n",
      "[93]\tvalid_0's multi_logloss: 1.30004\n",
      "[94]\tvalid_0's multi_logloss: 1.30007\n",
      "[95]\tvalid_0's multi_logloss: 1.29993\n",
      "[96]\tvalid_0's multi_logloss: 1.29973\n",
      "[97]\tvalid_0's multi_logloss: 1.29961\n",
      "[98]\tvalid_0's multi_logloss: 1.29941\n",
      "[99]\tvalid_0's multi_logloss: 1.29913\n",
      "[100]\tvalid_0's multi_logloss: 1.29876\n",
      "[101]\tvalid_0's multi_logloss: 1.29835\n",
      "[102]\tvalid_0's multi_logloss: 1.29806\n",
      "[103]\tvalid_0's multi_logloss: 1.29793\n",
      "[104]\tvalid_0's multi_logloss: 1.29781\n",
      "[105]\tvalid_0's multi_logloss: 1.29762\n",
      "[106]\tvalid_0's multi_logloss: 1.29735\n",
      "[107]\tvalid_0's multi_logloss: 1.29715\n",
      "[108]\tvalid_0's multi_logloss: 1.29692\n",
      "[109]\tvalid_0's multi_logloss: 1.29692\n",
      "[110]\tvalid_0's multi_logloss: 1.29682\n",
      "[111]\tvalid_0's multi_logloss: 1.29661\n",
      "[112]\tvalid_0's multi_logloss: 1.29659\n",
      "[113]\tvalid_0's multi_logloss: 1.29666\n",
      "[114]\tvalid_0's multi_logloss: 1.29637\n",
      "[115]\tvalid_0's multi_logloss: 1.29623\n",
      "[116]\tvalid_0's multi_logloss: 1.2963\n",
      "[117]\tvalid_0's multi_logloss: 1.29644\n",
      "[118]\tvalid_0's multi_logloss: 1.29645\n",
      "[119]\tvalid_0's multi_logloss: 1.2963\n",
      "[120]\tvalid_0's multi_logloss: 1.29602\n",
      "[121]\tvalid_0's multi_logloss: 1.29612\n",
      "[122]\tvalid_0's multi_logloss: 1.29632\n",
      "[123]\tvalid_0's multi_logloss: 1.29643\n",
      "[124]\tvalid_0's multi_logloss: 1.29642\n",
      "[125]\tvalid_0's multi_logloss: 1.29624\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's multi_logloss: 1.29602\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2dbc1daafe5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         print(\"Fold {}: accuracy = {:.1f}%, kappa = {:.4f}  (no boundary adjustment)\".format(idx,\n\u001b[0;32m---> 67\u001b[0;31m                                 \u001b[0;36m100.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                                 cohen_kappa_score(y_val, y_pred, weights='quadratic')))\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# create dataset for lightgbm\n",
    "\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class' : 5,\n",
    "    'metric': {'softmax'},\n",
    "    'num_leaves': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define what will be the final predicted train and test values\n",
    "train_meta = np.zeros(y.shape)\n",
    "test_meta = np.zeros(test_X.shape[0])\n",
    "\n",
    "#SMOTE over samplingg\n",
    "#sm = SMOTE(random_state=12, kind='regular')\n",
    "\n",
    "\n",
    "# Divide the training data into k-folds, k=4 here.\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=1812).split(X, y))\n",
    "\n",
    "#splits = list(GroupKFold(n_splits=4).split(X, y, X['RescuerID']))\n",
    "\n",
    "# Loop over the folds and fit the model to the fold's training data.\n",
    "# Then evaluate that model on i) the validation data of that fold, \n",
    "# and ii) on all of the test data.\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        # The training and validation sets for this fold\n",
    "        X_train = X.iloc[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_val = X.iloc[valid_idx]\n",
    "        y_val = y[valid_idx]\n",
    "        \n",
    "        # create dataset for lightgbm\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "        \n",
    "        #x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "        #X_train,y_train = x_train_res,y_train_res\n",
    "        \n",
    "        # Fit the model\n",
    "        #clf.fit(X_train, y_train)\n",
    "        print('Starting training...')\n",
    "        # train\n",
    "        clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=300,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "        \n",
    "        # Look at the validation kappa and accuracy with classes right from the model\n",
    "        y_pred = clf.predict(X_val)\n",
    "        print(\"Fold {}: accuracy = {:.1f}%, kappa = {:.4f}  (no boundary adjustment)\".format(idx,\n",
    "                                100.0*accuracy_score(y_val, y_pred),     \n",
    "                                cohen_kappa_score(y_val, y_pred, weights='quadratic')))\n",
    "        #\n",
    "        # Assign real-valued classes in addition to the integer classes of y_pred.\n",
    "        # Start with the predicted probabilities by class\n",
    "        y_probs = clf.predict_proba(X_val)\n",
    "        # and get the class values (use a copy incase we change values)\n",
    "        class_vals = clf.classes_.copy()\n",
    "        # Change the ordinal weight of class 0 to be -1 as suggested by the plot in discussion:\n",
    "        # https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76265\n",
    "        # Does mot make much difference, though.\n",
    "        class_vals[0] = -1\n",
    "        # Create the float class values as the probability-weighted class\n",
    "        # Here a python \"list comprehension\" is used rather than a loop.\n",
    "        y_floats = [sum(y_probs[ix]*class_vals) for ix in range(len(y_probs[:,0]))]\n",
    "        #   \n",
    "        # Save these y_float values instead of the y_pred integers;\n",
    "        ##train_meta[valid_idx] = y_pred.reshape(-1)\n",
    "        train_meta[valid_idx] = y_floats\n",
    "        # the predictions for just this validation fold are saved in the train_meta array;\n",
    "        # looping over all folds will provide one prediction for each training sample.\n",
    "\n",
    "        # Now use this fold's same model to generate Test predictions.\n",
    "        ##y_test = clf.predict(test_X)\n",
    "        # Instead of integer classes, get the predicted probabilites\n",
    "        test_probs = clf.predict_proba(test_X)\n",
    "        # and turn these into float class values.\n",
    "        # Unlike the validation case, we get a test prediction from every fold,\n",
    "        # so those float predictions are averaged. python list comprehension is used again.\n",
    "        ##test_meta += y_test.reshape(-1) / len(splits)\n",
    "        test_meta += np.array([sum(test_probs[ix]*class_vals) for\n",
    "                               ix in range(len(test_probs[:,0]))]) / len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n",
    "    \"\"\"\n",
    "    Find boundary values for y_pred to match the known y class percentiles.\n",
    "    Returns N-1 boundaries in y_pred values that separate y_pred\n",
    "    into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n",
    "    Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n",
    "    \"\"\"\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        # adjust the number of class 0 predictions?\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0) :\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def assign_class(y_pred, boundaries):\n",
    "    \"\"\"\n",
    "    Given class boundaries in y_pred units, output integer class values\n",
    "    \"\"\"\n",
    "    y_classes = np.zeros(len(y_pred))\n",
    "    for iclass, bound in enumerate(boundaries):\n",
    "        y_classes[y_pred >= bound] = iclass + 1\n",
    "    return y_classes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best kappa for class0 fraction = 0.3900\n"
     ]
    }
   ],
   "source": [
    "# This cell calculates and plots the kappa (and MSE) vs the class0 fraction adjustment.\n",
    "# Note that MSE prefers (lower MSE) a class0 fraction near/at 0,\n",
    "# whereas kappa prefers (higher kappa) a fraction near 1.\n",
    "# Then the class0 fraction that gives best training kappa is selected.\n",
    "train_y = y\n",
    "df_train = train_copy_df\n",
    "# Save values of kappa, MSE, and accuracy vs the class0 fraction\n",
    "kappas = []\n",
    "mses = []\n",
    "accurs = []\n",
    "# fractions to try... (could go larger than 1 if desired.)\n",
    "cl0fracs = np.array(np.arange(0.01,1.001,0.01))\n",
    "for cl0frac in cl0fracs:\n",
    "    boundaries = get_class_bounds(train_y, train_meta, class0_fraction=cl0frac)\n",
    "    train_meta_ints = assign_class(train_meta, boundaries)\n",
    "    kappa = cohen_kappa_score(df_train['AdoptionSpeed'], train_meta_ints, weights='quadratic')\n",
    "    kappas.append(kappa)\n",
    "    mse = mean_squared_error(df_train['AdoptionSpeed'], train_meta_ints)\n",
    "    mses.append(mse)\n",
    "    accur = accuracy_score(df_train['AdoptionSpeed'], train_meta_ints)\n",
    "    accurs.append(accur)\n",
    "    \n",
    "# Use the class0 fraction that gives the highest training kappa\n",
    "ifmax = np.array(kappas).argmax()\n",
    "cl0frac = cl0fracs[ifmax]\n",
    "\n",
    "print(\"Best kappa for class0 fraction = {:.4f}\".format(cl0frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class0_fraction = 0.3900, gives boundaries:\n",
      "[1.3889777138829231, 2.069707206916064, 2.469525784254074, 2.8838150026276708]\n",
      "Adjusted boundaries give:\n",
      "kappa = 0.4427  (with accuracy = 40.4%)\n"
     ]
    }
   ],
   "source": [
    "# Can skip the class0_fraction adjustment and plotting cells above;\n",
    "# can delete those two cells and just uncomment this line:\n",
    "##cl0frac = 1.0\n",
    "\n",
    "print(\"Using class0_fraction = {:.4f}, gives boundaries:\".format(cl0frac))\n",
    "boundaries = get_class_bounds(train_y, train_meta, class0_fraction=cl0frac)\n",
    "print(boundaries)\n",
    "\n",
    "train_meta_ints = assign_class(train_meta, boundaries)\n",
    "kappa = cohen_kappa_score(train_y, train_meta_ints, weights='quadratic')\n",
    "\n",
    "print(\"Adjusted boundaries give:\")\n",
    "print(\"kappa = {:.4f}  (with accuracy = {:.1f}%)\".format(kappa,\n",
    "                                100.0*accuracy_score(train_y, train_meta_ints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix - Columns are prediced 0, predicted 1, etc.\n",
      "\n",
      "[[  30  143  100   75   62]\n",
      " [  80 1244  955  493  318]\n",
      " [  31 1107 1381  882  636]\n",
      " [  11  577 1025  936  710]\n",
      " [   7  270  576  873 2471]]\n",
      "\n",
      "\n",
      "40.43% = 6062/14993 are on the diagonal (= accuracy)\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "con_mat = confusion_matrix(train_y, train_meta_ints)\n",
    "\n",
    "# Look at the number that are on the diagonal (exact agreement)\n",
    "diag = 0.0\n",
    "for id in range(5):\n",
    "    diag += con_mat[id,id]\n",
    "print(\"\\nConfusion matrix - Columns are prediced 0, predicted 1, etc.\\n\")\n",
    "print(con_mat)\n",
    "print(\"\")\n",
    "print(\"\\n{2:.2f}% = {0}/{1} are on the diagonal (= accuracy)\".format(\n",
    "        int(diag), con_mat.sum(), 100.0*diag/con_mat.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378fcc4fc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73c10e136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72000c4c5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e147a4b9f</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43fbba852</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77a490ec9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28c4b1b13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d1eada628</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d134dec34</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcd464bb8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  378fcc4fc              1\n",
       "1  73c10e136              4\n",
       "2  72000c4c5              4\n",
       "3  e147a4b9f              4\n",
       "4  43fbba852              4\n",
       "5  77a490ec9              4\n",
       "6  28c4b1b13              4\n",
       "7  d1eada628              3\n",
       "8  d134dec34              4\n",
       "9  bcd464bb8              2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta_ints = assign_class(test_meta, boundaries)\n",
    "PetID = test_copy_df['PetID']\n",
    "testAdoptionSpeed = pd.Series(data=test_meta_ints, name=\"AdoptionSpeed\")\n",
    "# Preparing data for Submission\n",
    "Submission = pd.concat([PetID,testAdoptionSpeed],axis=1)\n",
    "Submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.to_csv(\"Submission10.csv\", index=False)\n",
    "\n",
    "#submission 9 is with depth 10\n",
    "#submission 10 is depth 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
